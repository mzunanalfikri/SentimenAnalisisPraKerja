{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_json(\"#KartuPraKerja/#KartuPraKerja.json\", lines = True)\n",
    "data2 = pd.read_json(\"#PraKerja/#PraKerja.json\", lines = True)\n",
    "data3 = pd.read_json(\"Kartu/KartuPraKerja.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1[\"text\"][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.append(data2[\"text\"][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.append(data3[\"text\"][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Ini cuma proyek. Makanya gak pernah beres.\n",
      "\n",
      "#kartuprakerja https://t.co/sEDIThxakl\n",
      "===================\n",
      "Alhmdulillah lulus..trima kasih @ruangguru @skillacademy_id \n",
      "#kartuprakerja @jokowi ...semoga bermanfaat https://t.co/AecvTJ9elh\n",
      "===================\n",
      "RT @joeyakarta: Saya rutin memberikan pelatihan memancing scr online pada kucing-kucing saya. Menyenangkan memang. Namun, jelas tidak bergu…\n",
      "===================\n",
      "RT @anakpresidenJKW: Di hari pendidikan tahun ini... hanya RuangGuru yang Kaya Raya.. 5,6 Trilyun.. Mundur Mu demi uang belvara\n",
      ".\n",
      "Sementara…\n",
      "===================\n",
      "RT @dzulfian: Tidak ada guna @AdamasBelva mundur dari stafsus krn hmpir 70% proyek kartu #prakerja gelombang 1 dikuasai @ruangguru . DPR su…\n",
      "===================\n",
      "RT @dzulfian: Tidak ada guna @AdamasBelva mundur dari stafsus krn hmpir 70% proyek kartu #prakerja gelombang 1 dikuasai @ruangguru . DPR su…\n",
      "===================\n",
      "Please evaluate and rebuild these Kartu Pra Kerja stuffs. https://t.co/OHxshf78vr https://t.co/KsBNA549N7\n",
      "===================\n",
      "RT @HukumDan: @haikal_hassan Salah satu orang yg kebagian jatah 5 trilliun lebih proyek kartu pra kerja. \n",
      "Miris https://t.co/EzJTqLrCLP\n",
      "===================\n",
      "RT @panca66: Mau tahu modus pelaku kartu pra kerja menggangsir uang rakyat. Simak penjelasan mas Agustinus ini. Benar2 garong 4.0. Dan itu…\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print (\"===================\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9./_]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "pat3 = r'#[A-Za-z0-9./_]+'\n",
    "pat4 = r'RT'\n",
    "combined_pat = r'|'.join((pat1,pat2,pat3,pat4))\n",
    "stop_words = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, \"\", souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    filtered_words = [w for w in words if not w in stop_words]\n",
    "    return (\" \".join(filtered_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_text = data1[\"text\"]\n",
    "data2_text = data2[\"text\"]\n",
    "data3_text = data3[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Ini cuma proyek. Makanya gak pernah beres.\\n\\n...\n",
       "1       Alhmdulillah lulus..trima kasih @ruangguru @sk...\n",
       "2       RT @joeyakarta: Saya rutin memberikan pelatiha...\n",
       "3       RT @joeyakarta: \"Tak ada makan siang gratis!\"\\...\n",
       "4       Hak ada di penerima. Kalau meragukan kredibili...\n",
       "5       RT @dzulfian: 'Jgn sampai Pak @jokowi dibohong...\n",
       "6       Kuy isi waktu senggang dg gabung grup chat sup...\n",
       "7       ;-) pengen tahu aja nih si @AdamasBelva n komp...\n",
       "8       RT @beritaKBR: \"@KomnasHAM  menilai, kartu pra...\n",
       "9       Udah llos kartu prakerja? Atau msh nunggu peng...\n",
       "10      Udah lolos kartu prakerja? Atau msh nunggu pen...\n",
       "11      Buat yg belum atau masi mau pilih pelatihannya...\n",
       "12      [PENTING]\\nPendaftaran prakerja Gel-3 sudah tu...\n",
       "13      RT @dzulfian: Penelitian terbaru  @IndefEconom...\n",
       "14      JAKARTA, https://t.co/vuXwLcgDO7 - Menteri Keu...\n",
       "15      Baca Berita katanya tgl 1 Mei bakal cair tapi ...\n",
       "16      Upaya Pemerintah untuk terus meningkatkan kese...\n",
       "17      RT @joeyakarta: Saya rutin memberikan pelatiha...\n",
       "18      Hallo, adakah yg sudah Lolos atau baru daftar ...\n",
       "19      gelombang satu aja sampai skrg belum kelar2, g...\n",
       "20      RT @dzulfian: 'Jgn sampai Pak @jokowi dibohong...\n",
       "21      RT @yosnggarang: Agustinus Edy Kristianto itu ...\n",
       "22      Tidak Seperti Indonesia, Puluhan Negara Ganden...\n",
       "23      RT @antaranews: \"Pemerintah tidak perlu memaks...\n",
       "24      RT @dzulfian: Penelitian terbaru  @IndefEconom...\n",
       "25      RT @anas_achmad_: @killthedj @mas__piyuuu Keti...\n",
       "26      RT @yosnggarang: Agustinus Edy Kristianto itu ...\n",
       "27      RT @Palistian_: Kamu Sudah daftar #KartuPraker...\n",
       "28      RT @dzulfian: 'Jgn sampai Pak @jokowi dibohong...\n",
       "29      RT @dzulfian: Penelitian terbaru  @IndefEconom...\n",
       "                              ...                        \n",
       "1556    Hai Ini tampilan buat yg lolos prakerja gelomb...\n",
       "1557    Sesulit inikah mendapat kartu prakerja ? Sudah...\n",
       "1558    RT @twitpos: Tak Perlu Kartu Prakerja, Ini Daf...\n",
       "1559    ini konyol sekali, khursus yang didapat dari k...\n",
       "1560    RT @twitpos: Tak Perlu Kartu Prakerja, Ini Daf...\n",
       "1561    RT @hrdbacot: Trid kursus online rekomendasi m...\n",
       "1562    Halo warga twitter ✨\\nSudah ada yg lulus gelom...\n",
       "1563    Mau kuota gratis Indosat Ooredoo!?\\n\\nKalo mau...\n",
       "1564    WHAT? GELOMBANG DUA KARTU PRAKERJA PADA GA LOL...\n",
       "1565    Buat yang masih bingung ttg mekanisme Prakerja...\n",
       "1566    RT @hrdbacot: Trid kursus online rekomendasi m...\n",
       "1567    RT @KontanNews: Polemik baru: Ruangguru ternya...\n",
       "1568    Siapa nih yang udh lulus? Tp bingung pilih pel...\n",
       "1569    RT @beningtirta: Program #KartuPrakerja bisa d...\n",
       "1570    Problem solving kali ini adalah.....\\n.\\n.\\n.\\...\n",
       "1571    prgram pemerintah yg dibuat hnya tuk kepenting...\n",
       "1572    Kartu Pra Kerja Bisa Didapat Secara Offline, I...\n",
       "1573    RT @BatakMJS: #kartuprakerja sudah terrealisas...\n",
       "1574    RT @hrdbacot: Trid kursus online rekomendasi m...\n",
       "1575    RT @hrdbacot: Trid kursus online rekomendasi m...\n",
       "1576    PENTING ‼️‼️\\nTeman² yg ikut program Kartu Pra...\n",
       "1577    RT @dyahsulis6: Yang pingin tanya2 tentang kar...\n",
       "1578    RT @tvOneNews: Agustinus, Pemred https://t.co/...\n",
       "1579    RT @beningtirta: Program #KartuPrakerja bisa d...\n",
       "1580    RT @Lhakim_Lh: @bachtiaaaaar apaapan ini, seti...\n",
       "1581    Salah apa diriku padamuu #kartuprakerja https:...\n",
       "1582    Tolong yg bisa peduli bantuannya #kartuprakerj...\n",
       "1583    RT @tvOneNews: Agustinus, Pemred https://t.co/...\n",
       "1584    RT @beningtirta: Program #KartuPrakerja bisa d...\n",
       "1585    RT @beningtirta: Program #KartuPrakerja bisa d...\n",
       "Name: text, Length: 1586, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_clean = list(dict.fromkeys(data1_text))\n",
    "data2_clean = list(dict.fromkeys(data2_text))\n",
    "data3_clean = list(dict.fromkeys(data3_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1586\n",
      "753\n",
      "===============\n",
      "2745\n",
      "504\n",
      "=============\n",
      "7800\n",
      "1375\n"
     ]
    }
   ],
   "source": [
    "print(len(data1_text))\n",
    "print(len(data1_clean))\n",
    "print(\"===============\")\n",
    "print(len(data2_text))\n",
    "print(len(data2_clean))\n",
    "print(\"=============\")\n",
    "print(len(data3_text))\n",
    "print(len(data3_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7830351990767456"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (len(data1_clean)+len(data2_clean)+len(data3_clean))/(len(data1)+len(data2)+len(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data1_clean + data2_clean + data3_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2632"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Sentimen dengan TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(all_data))):\n",
    "    all_data[i] = cleaner(all_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pos = 0\n",
    "count_neg = 0\n",
    "count_netral = 0\n",
    "not_valid = 0\n",
    "polaritas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polarity = TextBlob(\"terimakasih\").translate(from_lang='id', to='en').sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2632"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_data = ['' for i in range(len(all_data))]\n",
    "trans = Translator()\n",
    "# trans.translate(\"halo nama saya zunan\", src='id',  dest='en').text\n",
    "for i in range(len(all_data)):\n",
    "    try :\n",
    "        translate_data[i] = trans.translate(all_data[i], src='id',  dest='en').text\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"startinggg\")\n",
    "for i in tqdm(range(len(all_data))):\n",
    "#     print(i)\n",
    "    try:\n",
    "        polarity = TextBlob(translate_data[i]).sentiment.polarity\n",
    "        polaritas.append(polarity)\n",
    "        if polarity>0:\n",
    "            count_pos += 1\n",
    "        elif polarity<0:\n",
    "            count_neg += 1\n",
    "        else:\n",
    "            count_netral += 1\n",
    "    except:\n",
    "        not_valid += 1\n",
    "        polaritas.append(999)\n",
    "        print(i)\n",
    "print(\"FINISHH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"startinggg\")\n",
    "for i in tqdm(range(len(all_data))):\n",
    "#     print(i)\n",
    "    try:\n",
    "        polarity = TextBlob(all_data[i]).translate(from_lang='id', to='en').sentiment.polarity\n",
    "        polaritas.append(polarity)\n",
    "        if polarity>0:\n",
    "            count_pos += 1\n",
    "        elif polarity<0:\n",
    "            count_neg += 1\n",
    "        else:\n",
    "            count_netral += 1\n",
    "    except:\n",
    "        not_valid += 1\n",
    "        polaritas.append(999)\n",
    "print(\"FINISHH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('positif : ', count_pos)\n",
    "print('negeatif : ', count_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2632"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_clean = all_data.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8b0431bf0009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "len(all_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafix = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafix.to_csv(\"data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data_clean_translate.csv' does not exist: b'data_clean_translate.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-cec444d1ac31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_fix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data_clean_translate.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\azpalace\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azpalace\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azpalace\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azpalace\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azpalace\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data_clean_translate.csv' does not exist: b'data_clean_translate.csv'"
     ]
    }
   ],
   "source": [
    "data_fix = pd.read_csv(\"data_clean_translate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
